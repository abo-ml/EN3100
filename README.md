# Design and Application of Machine Learning Models for Financial Market Prediction: A Comparative Study of Traditional and Deep Learning Approaches

## Overview
This repository accompanies an academic dissertation exploring a progressive sequence of forecasting and trading system designs for liquid financial assets. The project walks through five model iterations ranging from linear baselines to attention-based ensembles with risk-aware position sizing. The codebase emphasises reproducible research, walk-forward validation, and extensibility toward microstructure, sentiment, and reinforcement learning enhancements.

The default assets include:
- **AAPL** (US equity)
- **EURUSD=X** (FX proxy via Yahoo Finance)
- **XAUUSD=X** (Gold spot proxy)
- **^GSPC** (S&P 500 index benchmark)

All components operate on daily OHLCV bars sourced via `yfinance`. Hooks are provided for alternative data vendors (Alpha Vantage, Polygon, Interactive Brokers) and advanced modules (order flow imbalance, ICT/SMT liquidity concepts, reinforcement learning). TODO markers highlight where sensitive credentials, private market data, or broker integrations must be supplied by the student.

## Scope / decision gate
The mixed-asset workflow remains the default path for the dissertation. The new per-asset and optional 20-stock equity universe flows are additive experiment tracks that reuse the same walk-forward protocol and feature set. Use them when you want asset-specific diagnostics or a broader equity cross-section; otherwise stick with the baseline panel workflow for continuity with earlier results.

## Repository Structure
```
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/          # CI/CD workflows (tests.yml)
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py    # Optional visualisation UI
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ universe.yaml       # Config-driven pipeline settings
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Downloaded OHLCV data
â”‚   â””â”€â”€ processed/          # Aligned and feature-engineered datasets
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ COLAB_RUNBOOK.md    # Google Colab quick-start pipelines
â”‚   â”œâ”€â”€ EQUATIONS.md        # Mathematical reference for all formulas
â”‚   â”œâ”€â”€ EXPERIMENTS.md      # Experiment matrix and validation notes
â”‚   â””â”€â”€ local_setup.md      # Local and Colab setup guide
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Iteration_1.ipynb.py
â”‚   â”œâ”€â”€ Iteration_2.ipynb.py
â”‚   â”œâ”€â”€ Iteration_3.ipynb.py
â”‚   â”œâ”€â”€ Iteration_4.ipynb.py
â”‚   â”œâ”€â”€ Iteration_5.ipynb.py
â”‚   â””â”€â”€ final_comparison.ipynb.py
â”œâ”€â”€ project_assets/         # Project management files (Gantt, Risk Register)
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ figures/            # Generated plots (created at runtime)
â”‚   â”œâ”€â”€ iteration_1_results.md
â”‚   â”œâ”€â”€ iteration_1_1_svr_results.md      # Generated by iteration1_1_svr.py
â”‚   â”œâ”€â”€ iteration_2_results.md
â”‚   â”œâ”€â”€ iteration_2_1_lightgbm_results.md # Generated by iteration2_1_lightgbm.py
â”‚   â”œâ”€â”€ iteration_3_results.md
â”‚   â”œâ”€â”€ iteration_4_results.md
â”‚   â””â”€â”€ iteration_5_results.md
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ plot_walk_forward.py
â”‚   â””â”€â”€ smoke_check.sh      # Basic syntax/import checks
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ advanced/           # Advanced modules (order flow, pattern recognition, RL)
â”‚   â”œâ”€â”€ data/               # Data acquisition & alignment
â”‚   â”œâ”€â”€ evaluation/         # Metrics, walk-forward, reporting utilities
â”‚   â”œâ”€â”€ experiments/        # Per-asset & equity universe evaluation scripts
â”‚   â”œâ”€â”€ features/           # Feature engineering and target construction
â”‚   â”œâ”€â”€ models/             # Iteration-specific training scripts
â”‚   â”œâ”€â”€ risk/               # Monte Carlo risk analysis
â”‚   â”œâ”€â”€ universe/           # S&P 500 universe utilities
â”‚   â””â”€â”€ utils/              # Shared path utilities
â”œâ”€â”€ tests/                  # Unit tests for data downloads and advanced modules
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ md_to_doc_pdf.py    # Report conversion utility
â”œâ”€â”€ market_forecasting.py   # Standalone demonstration module
â”œâ”€â”€ CHANGELOG.md            # Release notes
â”œâ”€â”€ MERGE_GUIDE.md          # Conflict resolution guidance
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## API Requirements Summary

This section summarizes the APIs used by the project and which are required vs optional:

### Required APIs

| API | Purpose | Required For | Environment Variable |
|-----|---------|--------------|---------------------|
| **Alpha Vantage** | Daily OHLCV data for equities and FX | Core data download (unless using yfinance only) | `ALPHAVANTAGE_API_KEY` (preferred) or `ALPHA_VANTAGE_API_KEY` |

> **Note:** If both `ALPHAVANTAGE_API_KEY` and `ALPHA_VANTAGE_API_KEY` are set, `ALPHAVANTAGE_API_KEY` takes precedence.
> 
> **Note:** Known premium-only tickers (e.g., AAPL, MSFT, GOOGL, AMZN, META, TSLA, NVDA) automatically skip Alpha Vantage and use yfinance/Stooq instead to avoid premium endpoint errors.

### Optional APIs (for enhanced functionality)

| API | Purpose | Use Case | Environment Variable |
|-----|---------|----------|---------------------|
| **News API** | News headlines for sentiment analysis | Enhanced sentiment features | `NEWSAPI_KEY` |
| **FRED** | Federal Reserve economic data | Macro factor features | `FRED_API_KEY` |
| **Alpaca** | Real-time order book data | Order flow features, live trading | `APCA_API_KEY_ID`, `APCA_API_SECRET_KEY` |
| **Binance** | Crypto order book data | Crypto order flow features | `BINANCE_API_KEY`, `BINANCE_API_SECRET` |
| **Stooq** (via pandas_datareader) | Free OHLCV data fallback | Fallback when Alpha Vantage and yfinance fail | No API key required |

> **Quick Reference â€“ Environment Variables:**
> 
> | Variable | Purpose |
> |----------|---------|
> | **`ALPHAVANTAGE_API_KEY`** | Alpha Vantage API key (primary data provider) |
> | **`APCA_API_KEY_ID`** | Alpaca API key ID (for order book data) |
> | **`APCA_API_SECRET_KEY`** | Alpaca API secret key |
> | **`FRED_API_KEY`** | FRED API key (for macro data) |
>
> ```bash
> export ALPHAVANTAGE_API_KEY="your-alpha-vantage-key"
> export FRED_API_KEY="your-fred-key"            # optional, for macro data
> export APCA_API_KEY_ID="your-alpaca-key"       # optional, for order book
> export APCA_API_SECRET_KEY="your-alpaca-secret"
> export APCA_API_BASE_URL="https://paper-api.alpaca.markets"
> ```

### Minimum Viable Configuration

The pipeline can run with **just Yahoo Finance** (no API key required) by using `--provider yfinance`:
```bash
python -m src.data.download_data \
    --tickers AAPL EURUSD=X XAUUSD=X ^GSPC \
    --start 2013-01-01 --end 2023-12-31 \
    --provider yfinance
```
This is ideal for testing and development. For production/dissertation work, Alpha Vantage provides more reliable data.

### Provider Fallback Order

The data download module uses the following provider order by default:
1. **Alpha Vantage** (primary, requires `ALPHAVANTAGE_API_KEY`) - skipped for premium-only tickers
2. **yfinance** (fallback, no API key required)
3. **Stooq** (final fallback, requires `pandas_datareader>=0.10`)

For macro data (yields, VIX, etc.):
1. **FRED** (optional, requires `FRED_API_KEY`)
2. **yfinance** (fallback for common indices)

### What Works Without APIs

- âœ… Data alignment (`align_data.py`) - uses local files
- âœ… Feature engineering (`engineer_features.py`) - uses local files
- âœ… All model iterations (1-5) - uses processed features
- âœ… Monte Carlo risk analysis - uses iteration 5 outputs
- âœ… Per-asset and 20-stock evaluations - uses processed features
- âœ… Sentiment scoring (VADER/TextBlob) - local NLP libraries

### What Requires APIs

- ðŸ“¡ Live data download - requires Alpha Vantage or yfinance internet access
- ðŸ“¡ Real-time order book - requires Alpaca/Binance credentials
- ðŸ“¡ Live news sentiment - requires News API key
- ðŸ“¡ FRED macro data - requires FRED API key (optional, yfinance works as fallback)

## Getting Started
### 1. Set up the environment
```bash
python3.11 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2. Download market data
Export your Alpha Vantage API key (insert the key provided to you for coursework) and fetch daily OHLCV data.
```bash
export ALPHAVANTAGE_API_KEY="<YOUR_ALPHA_VANTAGE_KEY>"
# Optional: set the alternate variable name used in some notebooks/scripts.
export ALPHA_VANTAGE_API_KEY="$ALPHAVANTAGE_API_KEY"
```
Then pull the required tickers. Alpha Vantage is the default provider with automatic fallbacks to Yahoo Finance for symbols the API does not cover (e.g. broad market indices beginning with `^`).
Fetch daily OHLCV data and save to `data/raw/`.
```bash
python -m src.data.download_data \
    --tickers AAPL EURUSD=X XAUUSD=X ^GSPC \
    --start 2013-01-01 \
    --end 2023-12-31 \
    --provider alpha_vantage
```
Optional arguments allow changing the interval, retry policy, output format, and overriding the API key via `--api-key`. **TODO:** replace the dummy `fetch_orderbook_snapshot` implementation with a real broker API call (Interactive Brokers, Alpaca, etc.) when credentials are available.

### 3. Align data sources
Merge OHLCV, order flow (when available), sentiment, and macro features into a single dataset.
```bash
python -m src.data.align_data
```
This step produces `data/processed/combined_features.csv`. Extend `align_data.py` to ingest proprietary sentiment feeds, macro benchmarks, or alternative data.

### 4. Engineer features & targets
Generate technical, regime, and microstructure-inspired features along with supervised learning targets.
```bash
python -m src.features.engineer_features
```
Outputs include `data/processed/model_features.parquet`. TODO markers indicate where true VWAP/TWAP, chart pattern detection, and ICT/SMT annotations should be implemented.

#### Engineered Feature Set
The `engineer_features.py` module computes the following features per ticker:

| Feature | Description | Parameters |
|---------|-------------|------------|
| `return_1d` | Daily percentage return | â€” |
| `macd_line`, `macd_signal`, `macd_hist` | MACD indicator | fast=12, slow=26, signal=9 |
| `rsi_14` | Relative Strength Index | window=14 |
| `volatility_21` | Rolling realised volatility | window=21 |
| `volume_zscore_63` | Volume Z-score | window=63 |
| `tsmom_252` | Time-series momentum | lookback=252 |
| `swing_high`, `swing_low` | Local swing points | window=3 |
| `ofi`, `depth_ratio`, `bid_ask_spread` | Microstructure features | Zeros until order-book data integrated |
| `ma_bullish_crossover`, `ma_bearish_crossover` | Moving average crossovers | fast=10, slow=50 |
| `swing_high_flag`, `swing_low_flag` | Local high/low flags | window=3 |
| `pattern_head_shoulders`, `pattern_double_top`, `pattern_double_bottom` | Chart pattern flags | **Implemented**: Rule-based detection using peak/trough analysis (window=5, tolerance=0.02) |
| `ict_smt_asia` | ICT/SMT Asia session feature | Uses previous day's range as proxy; returns zeros when no breakout detected |
| `liquidity_grab` | Liquidity grab detection | Future work (volume_threshold=2.0, reversal_threshold=0.005, lookback=5) |
| `fvg` | Fair value gap detection | Future work (min_gap_percent=0.001, fill_lookforward=5) |
| `asia_breakout` | Asia session range breakout | Future work (asia_start=0, asia_end=6, london_start=8, london_end=12) |
| `realised_vol_bucket` | Volatility regime labels | Quantile buckets: low/medium/high at 33rd/66th percentiles |
| `drawdown` | Rolling drawdown from cumulative max | â€” |

Pattern recognition functions (`detect_head_and_shoulders`, `detect_double_top`) are implemented with rule-based algorithms that identify local maxima/minima and validate pattern constraints. Additional pattern detection functions (`flag_liquidity_grab`, `detect_fvg`, `asia_session_range_breakout`) are available in `src/advanced/pattern_recognition.py` and integrated into the pipeline with graceful fallback (returns zeros if `NotImplementedError` is raised). These are marked as **future work** pending further validation and tuning.

### 5. Run model iterations
Each iteration script performs walk-forward validation, trains the designated models, logs metrics to `reports/`, and saves diagnostic plots.
```bash
python -m src.models.iteration1_baseline
python -m src.models.iteration1_1_svr
python -m src.models.iteration2_ensemble
python -m src.models.iteration2_1_lightgbm
python -m src.models.iteration3_lstm
python -m src.models.iteration4_transformer
python -m src.models.iteration5_meta_ensemble
python -m src.risk.monte_carlo  # requires iteration 5 to have run
```
Refer to the notebooks in `notebooks/` for guided walkthroughs, narrative commentary, and exploratory analysis aligned with each iteration.

### 6. Running the project in Google Colab (or similar hosted notebooks)
When using Colab you typically upload or clone the repository into `/content`. The centralised
path utilities in `src/utils/paths.py` keep all artefacts anchored to the project root so the
commands below work regardless of the current working directory.

```python
# 1. (Optional) mount Google Drive if you want persistence beyond the session
from google.colab import drive
drive.mount("/content/drive")

# 2. Unzip or clone the repository into /content
!unzip -o "/content/EN3100.zip" -d "/content"  # adjust the filename as needed

# 3. Change into the project directory (with robust detection)
import os
# Detect the correct project directory after unzipping
possible_dirs = ['EN3100', 'EN3100-main', 'EN3100-master']
project_dir = None
for dirname in possible_dirs:
    path = f'/content/{dirname}'
    if os.path.isdir(path):
        project_dir = path
        break
if project_dir is None:
    print("ERROR: Could not find EN3100 project directory.")
    print("Contents of /content:", os.listdir('/content'))
    raise FileNotFoundError("EN3100 project directory not found. Check the extracted folder name.")
os.chdir(project_dir)
print(f"Changed to: {project_dir}")

# 4. Install dependencies
!pip install -r requirements.txt -q

# 5. Set credentials for data providers
os.environ["ALPHAVANTAGE_API_KEY"] = "<YOUR_KEY>"  # or ALPHA_VANTAGE_API_KEY

# 6. Run the same pipeline as documented above
!python -m src.data.download_data --tickers AAPL EURUSD=X XAUUSD=X ^GSPC \
    --start 2013-01-01 --end 2023-12-31 --provider alpha_vantage
!python -m src.data.align_data
!python -m src.features.engineer_features
!python -m src.models.iteration1_baseline
```

Key points for hosted notebooks:

- Always `cd` into the inner project folder before running CLI commands. With the shared
  path utilities, outputs will be directed to `data/` and `reports/` even if you launch a
  command one level higher, but switching directories avoids confusion when exploring files.
- Persist `data/` and `reports/` by copying them to Google Drive (or downloading them) at the
  end of the session if you want to resume without repeating the downloads.
- Keep API keys in environment variables or secret managers. Never hard-code credentials into
  notebooks before sharing or committing them.

### Pre-Colab Checklist

Before taking a zip of this repository into Google Colab, complete these steps:

1. **Prepare API Keys** (have these ready to paste into Colab):
   - Alpha Vantage API key (or use `--provider yfinance` for testing)
   - (Optional) News API key for enhanced sentiment
   - (Optional) FRED API key for macro data

2. **Clean Up Local Artifacts** (optional, reduces zip size):
   ```bash
   rm -rf data/raw/*.parquet data/raw/*.csv
   rm -rf data/processed/*.parquet data/processed/*.csv
   rm -rf reports/figures/*.png
   rm -rf __pycache__ .pytest_cache
   ```

3. **Create the Zip File**:
   ```bash
   cd ..  # move to parent directory
   zip -r EN3100.zip EN3100 -x "*.git*" -x "*__pycache__*" -x "*.pytest_cache*"
   ```

4. **In Colab, Run This Complete Pipeline**:
   ```python
   # Mount Drive and extract
   from google.colab import drive
   drive.mount('/content/drive')
   !unzip -o "/content/drive/MyDrive/EN3100.zip" -d "/content"
   %cd /content/EN3100

   # Install dependencies
   !pip install -r requirements.txt -q

   # Set API keys
   import os
   os.environ["ALPHAVANTAGE_API_KEY"] = "YOUR_KEY_HERE"  # Replace with your key

   # Run the full pipeline
   !python -m src.data.download_data --tickers AAPL EURUSD=X XAUUSD=X ^GSPC --start 2013-01-01 --end 2023-12-31 --provider alpha_vantage
   !python -m src.data.align_data
   !python -m src.features.engineer_features
   !python -m src.models.iteration1_baseline
   !python -m src.models.iteration1_1_svr
   !python -m src.models.iteration2_ensemble
   !python -m src.models.iteration2_1_lightgbm
   !python -m src.models.iteration3_lstm
   !python -m src.models.iteration4_transformer
   !python -m src.models.iteration5_meta_ensemble
   !python -m src.risk.monte_carlo
   !python -m src.experiments.per_asset_evaluation

   # For 20-stock evaluation (optional):
   !python -m src.experiments.download_equity_universe --n 20 --start 2013-01-01 --end 2023-12-31 --provider yfinance
   !python -m src.data.align_data --ticker-file data/reference/equity_universe_20.txt
   !python -m src.features.engineer_features
   !python -m src.experiments.evaluate_20_stock_all_iterations
   ```

5. **Save Results to Drive**:
   ```python
   !cp -r reports /content/drive/MyDrive/EN3100_reports/
   !cp -r data/processed /content/drive/MyDrive/EN3100_data/
   ```

## Per-asset evaluation (4-asset universe)
Run the existing iterations per ticker while keeping the same walk-forward settings and feature set:
```bash
python -m src.data.download_data --tickers AAPL EURUSD=X XAUUSD=X ^GSPC --start 2013-01-01 --end 2023-12-31 --provider alpha_vantage
python -m src.data.align_data --tickers AAPL EURUSD=X XAUUSD=X ^GSPC
python -m src.features.engineer_features
python -m src.experiments.per_asset_evaluation
```
The `--tickers`/`--ticker-file` flags on `align_data` let you restrict alignment without altering the default mixed-asset behaviour.

## Optional 20-stock S&P 500 experiment
Sample a reproducible equity universe, download data, align only those tickers, engineer features, and run per-stock reports:
```bash
python -m src.experiments.download_equity_universe --n 20 --start 2013-01-01 --end 2023-12-31 --provider yfinance
python -m src.data.align_data --ticker-file data/reference/equity_universe_20.txt
python -m src.features.engineer_features
python -m src.experiments.per_asset_equity_evaluation
```
Ticker lists are stored under `data/reference/`, with aligned data in `data/processed/` and plots in `reports/figures/`. The mixed-asset workflow stays unchanged if you skip these optional steps.

For a comprehensive evaluation across all iterations on the 20-stock universe:
```bash
python -m src.experiments.evaluate_20_stock_all_iterations
```
This produces summary statistics, pivot tables, and iteration comparisons for the equity universe.

## Config-driven runner (application layer)
Use the lightweight runner to drive the same pipelines from a config file:
```bash
python -m src.experiments.run_pipeline --config configs/universe.yaml
```
Adjust `configs/universe.yaml` to choose `mode` (`core4`, `sp500_sample`, or `custom`), provider, dates, and optional tags for outputs. The runner appends tags to report filenames when `--tag` is supplied, leaving defaults unchanged otherwise.

## Streamlit UI (optional visualisation)
Launch a simple UI to trigger the runner and view existing reports/figures:
```bash
streamlit run app/streamlit_app.py
```
The UI is for demonstration/visualisation only; dissertation results should continue to use the deterministic CLI flows above.

## Running Tests
Run the test suite with pytest to verify data downloads and advanced module functionality:
```bash
pip install pytest
pytest tests/ -v
```
Tests include:
- `test_download_data.py`: validates provider fallback and CLI parser
- `test_placeholders.py`: validates advanced modules (order flow, pattern detection, RL environment)

A smoke-check script is also available for quick syntax verification:
```bash
bash scripts/smoke_check.sh
```

## Iteration Roadmap
1. **Iteration 1 â€“ Linear Baselines:** Persistence, linear regression, and logistic regression models validate the pipeline and establish benchmark metrics.
2. **Iteration 1.1 â€“ SVR Baseline:** Extends Iteration 1 with an RBF Support Vector Regressor to benchmark kernel-based nonlinearity against the linear regressor on identical walk-forward splits.
3. **Iteration 2 â€“ Tree-Based Ensembles:** Random Forest, optional XGBoost, and SVM classifiers capture nonlinear relationships and deliver feature importance insights.
4. **Iteration 2.1 â€“ LightGBM Upgrade:** Extends Iteration 2 with a gradient-boosted tree regressor (LightGBM) plus feature importances, allowing a direct comparison to the Random Forest/XGBoost ensemble.
5. **Iteration 3 â€“ LSTM Sequence Model:** Recurrent neural networks learn temporal dependencies over rolling windows and introduce deep learning considerations (scaling, early stopping, regularisation).
6. **Iteration 4 â€“ Transformer Encoder:** Attention mechanisms handle long-range dependencies and multimodal inputs (price, sentiment, order flow) for enhanced forecasting power.
7. **Iteration 5 â€“ Meta-Ensemble with Risk Layer:** Stacks the strongest models, applies dynamic volatility-aware position sizing, and includes execution scheduling (VWAP/TWAP, pairs trading hook).
8. **Iteration 5 â€“ Monte Carlo Risk:** A standalone Monte Carlo module bootstraps Iteration 5 strategy returns to stress-test equity curve variability, drawdown risk, and tail outcomes.

### Model Hyperparameters

#### Iteration 1.1 â€“ SVR Tuning
The SVR model uses an RBF kernel with grid-search hyperparameter tuning on a validation tail (20% of training data):
| Parameter | Search Grid | Description |
|-----------|-------------|-------------|
| `C` | [1, 10, 100] | Regularisation parameter |
| `epsilon` | [0.001, 0.01, 0.1] | Epsilon-tube width |
| `gamma` | ["scale", 0.01, 0.1] | RBF kernel coefficient |

Selection criterion: lowest RMSE on the validation set.

#### Iteration 2.1 â€“ LightGBM Tuning
LightGBM uses gradient boosting with grid-search over:
| Parameter | Search Grid | Description |
|-----------|-------------|-------------|
| `num_leaves` | [15, 31, 63] | Maximum leaves per tree |
| `max_depth` | [-1, 6, 10] | Tree depth limit (-1 = no limit) |
| `learning_rate` | [0.01, 0.05, 0.1] | Boosting learning rate |
| `n_estimators` | [200, 500, 800] | Number of boosting rounds |

Base parameters: `objective="regression"`, `subsample=0.8`, `colsample_bytree=0.8`, `random_state=42`.

## Validation & Fairness Across Market Regimes
Robustness is evaluated via chronological walk-forward validation across all assets. Metrics are aggregated within each split to inspect performance under varying volatility regimes, drawdowns, and market conditions. The `engineer_features.py` module labels regimes (e.g., high/low volatility buckets, risk-off periods) enabling analysis of whether models degrade during stress events. Future work should further stratify results by asset class, liquidity profiles, and macroeconomic cycles to avoid regime overfitting.

## Credentials & Sensitive Data
- **Order book / Level 2 feeds:** Insert broker API credentials (Interactive Brokers TWS, Alpaca) where the TODO markers appear. Store keys securely (environment variables, secrets manager).
- **Sentiment APIs:** Add keys for Twitter, news providers, or alternative sentiment platforms via `os.environ["SENTIMENT_API_KEY"]`.
- **Execution / Trading APIs:** Connect the VWAP/TWAP planners to broker SDKs once paper trading permissions are granted.
- **Security:** Keep secrets in environment variables or a local `.env` file (already ignored in version control). Never commit credentials, and rotate any key that may have been exposed.

### Broker API Configuration
The order book integration supports two providers:

**Alpaca (for US equities):**
```bash
export APCA_API_KEY_ID="your-api-key"
export APCA_API_SECRET_KEY="your-secret-key"
export APCA_API_BASE_URL="https://paper-api.alpaca.markets"  # Optional, defaults to paper trading
```

**Binance (for crypto):**
```bash
export BINANCE_API_KEY="your-api-key"       # Optional for public endpoints
export BINANCE_API_SECRET="your-secret"      # Optional for public endpoints
```

### TODO checklist (API and private data wiring)
- `src/data/download_data.py` CLI: supply your Alpha Vantage key via `--api-key` or the `ALPHAVANTAGE_API_KEY/ALPHA_VANTAGE_API_KEY` environment variable when using the Alpha Vantage provider.
- `src/data/align_data.py`: pass `--sentiment-csv` or populate `fetch_sentiment_scores` with your API-backed sentiment feed.
- `src/advanced/sentiment.py`: implement the real sentiment loaders or API calls; keep credentials outside version control.

### If you hit a merge conflict on GitHub
- Prefer the version that keeps the Alpha Vantage+yfinance downloader and shared path utilities (for example, **accept the incoming change** in `src/data/download_data.py`).
- Confirm post-merge that data still lands in `data/raw` and reports in `reports/`, and re-run `python -m compileall src`.
- See `MERGE_GUIDE.md` for a concise checklist.

## Advanced Modules

### Pattern Recognition (`src/advanced/pattern_recognition.py`)
The pattern recognition module provides rule-based detectors for common trading patterns:

| Function | Description | Key Parameters | Status |
|----------|-------------|----------------|--------|
| `flag_liquidity_grab()` | Detects volume spikes with price reversals | `volume_threshold=2.0`, `reversal_threshold=0.005`, `lookback=5` | Future work |
| `detect_fvg()` | Identifies fair value gaps between candles | `min_gap_percent=0.001`, `fill_lookforward=5` | Future work |
| `asia_session_range_breakout()` | Marks breakouts from overnight range | `asia_start=0`, `asia_end=6`, `london_start=8`, `london_end=12` | Future work |

> **Note:** These functions are implemented but marked as **future work** pending further validation. They are integrated into the feature engineering pipeline with graceful fallback - if any function raises `NotImplementedError`, the pipeline returns zeros for that feature and continues without breaking.

**Example usage:**
```python
from src.advanced.pattern_recognition import flag_liquidity_grab, detect_fvg

# Detect liquidity grabs
liquidity_signals = flag_liquidity_grab(price_df, volume_threshold=2.0)

# Detect fair value gaps
fvg_signals = detect_fvg(price_df, min_gap_percent=0.001)
```

### Order Flow Scalping (`src/advanced/orderflow_scalping.py`)
The order flow module integrates with broker APIs to fetch order book data and generate scalping signals:

**Key components:**
- `fetch_orderbook_snapshot()`: Fetches top-N bid/ask prices from Alpaca or Binance
- `OrderFlowAlphaModel`: Generates signals based on OFI and momentum

**Example usage:**
```python
from src.advanced.orderflow_scalping import fetch_orderbook_snapshot, OrderFlowAlphaModel

# Fetch order book (requires API credentials)
snapshot = fetch_orderbook_snapshot("AAPL", top_n=10, provider="alpaca")
print(f"Mid price: {snapshot.mid_price}, Spread: {snapshot.spread}")

# Train alpha model
model = OrderFlowAlphaModel(ofi_weight=0.6, momentum_weight=0.4)
model.fit(historical_orderbook_df)

# Generate signal
signal = model.predict_signal(latest_ofi=0.1, spread=0.01, depth_ratio=0.55)
```

### Reinforcement Learning (`src/advanced/reinforcement_learning.py`)
Train RL agents using PPO or A2C algorithms from stable-baselines3:

**Components:**
- `TradingEnv`: Gymnasium environment simulating trading with transaction costs
- `TradingEnvConfig`: Configuration dataclass for environment parameters
- `train_rl_agent()`: Training function with customizable hyperparameters
- `evaluate_agent()`: Evaluation function returning Sharpe, returns, and drawdown

**CLI Usage:**
```bash
# Train PPO agent on AAPL data
python market_forecasting.py train-rl --tickers AAPL --timesteps 50000

# Train A2C agent with custom parameters
python market_forecasting.py train-rl \
    --tickers AAPL TSLA \
    --algorithm A2C \
    --timesteps 100000 \
    --window-size 60 \
    --output models/rl_agent.zip
```

**Python API:**
```python
from src.advanced.reinforcement_learning import TradingEnv, TradingEnvConfig, train_rl_agent

# Create environment
config = TradingEnvConfig(window_size=60, transaction_cost=0.0001)
env = TradingEnv(prices=price_array, features=feature_array, config=config)

# Train agent
model = train_rl_agent(env, algorithm="PPO", total_timesteps=10000)

# Use trained agent
obs, _ = env.reset()
action, _ = model.predict(obs)
```

## Reports & Interpretation
Each iteration stores Markdown summaries in `reports/iteration_X_results.md` and plots in `reports/figures/`. Review these artefacts alongside the notebooks to interpret model strengths, weaknesses, and improvement paths. Iteration 5 additionally logs cumulative PnL, Sharpe ratio, max drawdown, and hit rate for the dynamic strategy, while `src/risk/monte_carlo.py` produces equity/drawdown histograms and fan charts for stress-testing that strategy.

### Generated Figures
The following figures are created at runtime when running model iterations:

| Figure | Source | Description |
|--------|--------|-------------|
| `iteration1_pred_vs_actual.png` | Iteration 1 | Actual vs predicted returns |
| `iteration1_1_pred_vs_actual.png` | Iteration 1.1 | SVR actual vs predicted |
| `iteration2_1_pred_vs_actual.png` | Iteration 2.1 | LightGBM actual vs predicted |
| `iteration2_1_feature_importances.png` | Iteration 2.1 | LightGBM top-20 feature importances |
| `iteration5_equity_curve.png` | Iteration 5 | Strategy equity curve |
| `iteration5_mc_final_equity_hist.png` | Monte Carlo | Final equity distribution |
| `iteration5_mc_drawdown_hist.png` | Monte Carlo | Max drawdown distribution |
| `iteration5_mc_equity_fan.png` | Monte Carlo | Equity fan chart with percentiles |
| `per_asset_directional_accuracy.png` | Per-asset eval | Directional accuracy by ticker/model |
| `equity_avg_directional_accuracy.png` | Equity eval | 20-stock average directional accuracy |
| `equity_da_heatmap.png` | Equity eval | Ticker Ã— iteration accuracy heatmap |
| `walk_forward_splits.png` | 20-stock eval | Walk-forward split schematic |

## Mathematical Reference (All Equations)
Every transformation, feature, target, and metric used across the pipeline is written explicitly in `docs/EQUATIONS.md`. Consult that file when documenting methodology or validating that the implementation matches the theoretical definitions (e.g., RSI, MACD, OFI, walk-forward scaling, RMSE/MAE/RÂ², Sharpe/max drawdown, position sizing, Monte Carlo bootstrap).

## Additional Documentation
The `docs/` folder contains supplementary guides and references:
- **`COLAB_RUNBOOK.md`**: Ready-to-run pipelines for Google Colab (4-asset and 20-stock flows)
- **`EQUATIONS.md`**: Complete mathematical reference for all features, metrics, and transformations
- **`EXPERIMENTS.md`**: Experiment matrix detailing the three evaluation modes (mixed panel, per-asset, equity universe)
- **`local_setup.md`**: Step-by-step setup guide for Windows, macOS/Linux, and Colab environments

See also `CHANGELOG.md` for release notes and `MERGE_GUIDE.md` for conflict resolution guidance.

## Asset Scope Guidance
- **Single-asset runs (e.g., AAPL only):** useful for isolating model behaviour on one market, speeding up experimentation, and diagnosing feature relevance without cross-asset noise. Risk: overfitting to idiosyncratic patterns.
- **Single asset class (e.g., multiple equities or multiple FX pairs):** captures shared regime structure within a domain while keeping features comparable. Good middle ground for building specialised models per asset class.
- **Random sample of ~20 S&P 500 stocks:** broadens cross-sectional variation and can improve generalisation of tabular models (Linear/LightGBM) if features are scaled by ticker. It also enables pseudo cross-sectional learning but may dilute performance on any single name. When sampling, keep walk-forward splits chronological per ticker and consider per-ticker normalisation to avoid large-cap/low-vol names dominating the loss.

## Future Extensions
- **Sentiment Fusion:** Integrate natural language processing pipelines for tweets, macro headlines, and alternative data. Consider transformer-based text encoders feeding into the time-series models.
- **Advanced RL:** Extend the RL module with multi-agent setups, portfolio optimization, and risk-constrained policies.
- **Real-time Trading:** Connect the order flow module to live broker APIs for paper trading validation.

## Acknowledgements
This project leverages open-source libraries (`pandas`, `scikit-learn`, `tensorflow`, `yfinance`, `stable-baselines3`, `gymnasium`) and builds upon academic literature on time-series forecasting, financial econometrics, and algorithmic trading. The structure is designed for rigorous experimentation, reproducibility, and transparency required for a dissertation project.
