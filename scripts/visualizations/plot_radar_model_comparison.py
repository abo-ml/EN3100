#!/usr/bin/env python3
"""Prompt 4 — Radar Chart Comparing Top Models on Multiple Metrics.

Use iteration_summary.csv generated by the evaluation script.
Choose a few top iterations (e.g., "2.1", "3", "5") and plot their
normalised scores on multiple metrics (DA, RMSE, R²).

This radar chart shows trade-offs across several metrics for the
best-performing models.

Usage:
    python scripts/visualizations/plot_radar_model_comparison.py \
        --input results/20_stock/iteration_summary.csv \
        --models 2.1 3 5 \
        --output figures/radar_model_comparison.png
"""
from __future__ import annotations

import argparse
from pathlib import Path
from typing import List

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from src.utils import FIGURES_DIR, PROJECT_ROOT


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Generate radar chart comparing top models on multiple metrics"
    )
    parser.add_argument(
        "--input",
        type=Path,
        default=PROJECT_ROOT / "results" / "20_stock" / "iteration_summary.csv",
        help="Path to iteration_summary.csv",
    )
    parser.add_argument(
        "--models",
        nargs="*",
        default=["2.1", "3", "5"],
        help="List of model iterations to compare",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=FIGURES_DIR / "radar_model_comparison.png",
        help="Output figure path",
    )
    return parser.parse_args()


def plot_radar_model_comparison(
    input_path: Path, output_path: Path, models: List[str]
) -> None:
    """Generate radar chart comparing models on multiple normalized metrics."""
    summary = pd.read_csv(input_path)
    summary["iteration"] = summary["iteration"].astype(str)

    # Define metrics to plot (adjust names based on actual column names)
    metrics = ["mean_da", "mean_rmse", "mean_r2"]

    # Filter to only metrics that exist in the dataframe
    available_metrics = [m for m in metrics if m in summary.columns]
    if not available_metrics:
        # Try alternative column names
        alt_metrics = {
            "mean_da": ["mean_directional_accuracy", "directional_accuracy_mean"],
            "mean_rmse": ["rmse_mean"],
            "mean_r2": ["r2_mean"],
        }
        for m, alts in alt_metrics.items():
            for alt in alts:
                if alt in summary.columns and m not in available_metrics:
                    summary[m] = summary[alt]
                    available_metrics.append(m)
                    break

    if not available_metrics:
        raise ValueError(f"No suitable metrics found in {input_path}. Available columns: {summary.columns.tolist()}")

    # Normalize metrics (higher is better for DA & R2; lower is better for RMSE)
    normalized = {}
    for m in available_metrics:
        col_values = summary[m].dropna()
        if col_values.empty:
            continue
        if "rmse" in m.lower():
            # Invert RMSE (smaller is better)
            inv = col_values.max() - summary[m]
            max_inv = inv.max()
            normalized[m] = inv / max_inv if max_inv > 0 else inv
        else:
            max_val = summary[m].max()
            normalized[m] = summary[m] / max_val if max_val > 0 else summary[m]

    labels = available_metrics
    angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()

    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))

    for model in models:
        model_str = str(model)
        model_rows = summary[summary["iteration"] == model_str]
        if model_rows.empty:
            print(f"Warning: Model {model_str} not found in data")
            continue

        row_idx = model_rows.index[0]
        values = [normalized[m][row_idx] for m in labels]
        values += values[:1]  # Close the loop
        ax.plot(angles + [angles[0]], values, "o-", label=f"Model {model_str}", linewidth=2)
        ax.fill(angles + [angles[0]], values, alpha=0.1)

    ax.set_xticks(angles)
    ax.set_xticklabels(labels)
    ax.set_title("Normalized Performance Radar Chart")
    ax.legend(loc="upper right", bbox_to_anchor=(1.3, 1.1))
    plt.tight_layout()

    output_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(output_path, dpi=150, bbox_inches="tight")
    plt.close(fig)
    print(f"Saved radar chart to {output_path}")


def main() -> None:
    args = parse_args()
    plot_radar_model_comparison(args.input, args.output, args.models)


if __name__ == "__main__":
    main()
